# vLLM-FL Dispatch Configuration for CUDA
# Auto-loaded when running on NVIDIA GPU hardware

# Preferred default backend type: flaggems, vendor, reference
prefer: flagos

# Strict Mode:
#   true  = Raise an error immediately on failure; do not attempt other backends.
#   false = Attempt the next available backend in sequence upon failure (Default).
strict: false

# Vendor Whitelist (Optional, allows all if not set)
# allow_vendors:
#  - cuda

# Vendor Blacklist (Optional)
# deny_vendors:
#   - ascend

# Per-operator backend execution order (Optional)
# Only the backends listed here will be attempted, in the order specified.
#
# Supported tokens:
#   - flaggems      : Default FlagGems implementation (Triton)
#   - reference     : PyTorch reference implementation
#   - vendor        : Any available vendor backend (auto-detected)
#   - vendor:cuda   : CUDA-specific vendor backend
op_backends:
  # attention_backend: prioritize flaggems (Triton attention)
  attention_backend:
    - flagos
    - vendor
    - reference

  # rms_norm: prioritize flaggems (Triton)
  rms_norm:
    - flagos
    - vendor
    - reference

  # silu_and_mul: prioritize flaggems
  silu_and_mul:
    - flagos
    - vendor
    - reference

  # rotary_embedding: prioritize flaggems
  rotary_embedding:
    - flagos
    - vendor
    - reference

# FlagOS operator blacklist
flagos_blacklist:
  - to_copy
  - _to_copy
  - zeros
  - copy_
  - fill_scalar_
  - sum_dim
  - exponential_
  - mm
  - resolve_neg
  - resolve_conj
  - eq_scalar
  - floor_divide
  - cumsum
  - mul
  - reciprocal
  - repeat
  - randn
  - add
  - ge_scalar
  - sub
  - bitwise_and
  - bitwise_not
  - slice_scatter
  - fill_tensor_
  - conv1d
  - conv2d
  - uniform_
  - prod
  - max
  - amax
  - cat
  - stack
  - flatten
  - reshape
  - view
  - tensor

# OOT (Out-of-Tree) operator blacklist
# oot_blacklist: []
